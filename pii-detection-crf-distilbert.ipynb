{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7659420,"sourceType":"datasetVersion","datasetId":4459964}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CS 475/675 Machine Learning: Project\n## Goals:\n### 4.1 Must accomplish\n- Implement a robust data preprocessing pipeline to handle tokenization, feature extraction, and label encoding.\n- Develop and train a machine learning model capable of accurately detecting PII types in student essays, achieving a competitive score on the evaluation metric.\n- Generate predictions for the test set essays and submit them in the required format for evaluation.\n\n### 4.2 Expect to accomplish\n- Fine-tune the model architecture and hyperparameters to optimize performance on the provided training data.\n- Conduct error analysis and model interpretation to identify common misclassifications and areas for improvement.\n- Investigate the use of external datasets or pre-trained language models to enhance the model’s generalization capabilities.\n\n### 4.3 Would like to accomplish\n- Implement ensemble learning techniques, such as model averaging or stacking, to combine multiple base models and further boost detection accuracy and robustness.\n- Investigate methods for handling imbalance class distributions, particularly for rare PII types.\n- Develop visualization tools and techniques to facilitate the interpretation of model predictions.\n","metadata":{}},{"cell_type":"markdown","source":"# PreTrained DistilBERT Model","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers datasets accelerate","metadata":{"execution":{"iopub.status.busy":"2024-04-30T02:43:23.085875Z","iopub.execute_input":"2024-04-30T02:43:23.086780Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\nCollecting transformers\n  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\nCollecting accelerate\n  Downloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data loading","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict, load_metric\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:50:18.323531Z","iopub.execute_input":"2024-04-26T17:50:18.323861Z","iopub.status.idle":"2024-04-26T17:50:36.570034Z","shell.execute_reply.started":"2024-04-26T17:50:18.323832Z","shell.execute_reply":"2024-04-26T17:50:36.569210Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-26 17:50:28.709545: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-26 17:50:28.709647: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-26 17:50:28.841715: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport torch\nfrom transformers import DistilBertTokenizerFast\nfrom sklearn.model_selection import train_test_split\n\n# Load your dataset\nwith open('/kaggle/input/pii-detection-removal-from-educational-data/train.json', 'r') as file:\n    data = json.load(file)\n    \n# Extract tokens and labels from your data\ntokens = [entry['tokens'] for entry in data]\nlabels = [entry['labels'] for entry in data]\n\n# Create a dictionary for the Dataset\ndata_dict = {\n    'tokens': tokens,\n    'labels': labels\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:50:36.571173Z","iopub.execute_input":"2024-04-26T17:50:36.571948Z","iopub.status.idle":"2024-04-26T17:50:39.500121Z","shell.execute_reply.started":"2024-04-26T17:50:36.571918Z","shell.execute_reply":"2024-04-26T17:50:39.499238Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Convert to Hugging Face dataset format\ndataset = Dataset.from_dict(data_dict)\n\n# Split the dataset into training and validation sets\ntrain_test_split = dataset.train_test_split(test_size=0.2)\ndataset = DatasetDict(train=train_test_split['train'], validation=train_test_split['test'])\n\n# Define labels and map them\nlabel_list = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', \n               'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', \n               'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', \n               'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\nlabel2id = {label: i for i, label in enumerate(label_list)}\nid2label = {i: label for label, i in label2id.items()}","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:50:39.502901Z","iopub.execute_input":"2024-04-26T17:50:39.503596Z","iopub.status.idle":"2024-04-26T17:50:41.279846Z","shell.execute_reply.started":"2024-04-26T17:50:39.503558Z","shell.execute_reply":"2024-04-26T17:50:41.279101Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(id2label)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:50:41.281089Z","iopub.execute_input":"2024-04-26T17:50:41.281484Z","iopub.status.idle":"2024-04-26T17:50:41.287224Z","shell.execute_reply.started":"2024-04-26T17:50:41.281451Z","shell.execute_reply":"2024-04-26T17:50:41.286255Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_checkpoint = \"distilbert-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples['tokens'], \n        max_length=512,  # Set maximum sequence length (DistilBERT's typical max)\n        truncation=True,  # Ensure truncation is applied\n        padding='max_length',  # Apply padding\n        is_split_into_words=True\n    )\n    \n    labels = []\n    for i, doc_labels in enumerate(examples['labels']):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_id = None\n        label_ids = []\n        for word_id in word_ids:\n            # Only add labels for non-special tokens and avoid duplicated labels\n            if word_id is None:\n                label_ids.append(-100)  # For special tokens like [CLS], [SEP], [PAD]\n            elif word_id != previous_word_id:\n                label_ids.append(label2id.get(doc_labels[word_id], -100))\n            else:\n                # Use -100 for repeated subword tokens to ignore during loss calculation\n                label_ids.append(-100) \n            previous_word_id = word_id\n        labels.append(label_ids)\n        \n    tokenized_inputs['labels'] = labels\n    return tokenized_inputs\n\n\n# Apply tokenization and label alignment\ntokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True, \n                                remove_columns=dataset['train'].column_names)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:50:41.288387Z","iopub.execute_input":"2024-04-26T17:50:41.288681Z","iopub.status.idle":"2024-04-26T17:51:11.825588Z","shell.execute_reply.started":"2024-04-26T17:50:41.288656Z","shell.execute_reply":"2024-04-26T17:51:11.824259Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1c6fb1f336042d9af0eee9f984cd36b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79cba192898e45fd95958108efd3647c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08caa5ef896b4f079c1ac7d78966d36f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c93c58aadb945ad8bee1c6870a60c48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5445 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c853672d110943e197a772e947bf7a0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1362 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac95d29059745c09c8dd6b95b2763a0"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Collation and Model Building","metadata":{}},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, \n                                                        num_labels=len(label_list), \n                                                        id2label=id2label, label2id=label2id)\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:51:11.826818Z","iopub.execute_input":"2024-04-26T17:51:11.827118Z","iopub.status.idle":"2024-04-26T17:51:13.550939Z","shell.execute_reply.started":"2024-04-26T17:51:11.827091Z","shell.execute_reply":"2024-04-26T17:51:13.550054Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/263M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c9c75c2660a4f29b61e43fd8e7ba3a2"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training Metrics","metadata":{}},{"cell_type":"code","source":"!pip install evaluate\n!pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:51:13.552185Z","iopub.execute_input":"2024-04-26T17:51:13.552833Z","iopub.status.idle":"2024-04-26T17:51:43.620358Z","shell.execute_reply.started":"2024-04-26T17:51:13.552803Z","shell.execute_reply":"2024-04-26T17:51:43.619264Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.19.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.21.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.0.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=1cd8638270bb8359c76407975ef4825f7a54fad740d1b3e77a7a1476124ca42d\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Metrics for evaluation\nimport evaluate\nfrom seqeval.metrics import precision_score, recall_score, f1_score\nfrom seqeval.metrics import classification_report, accuracy_score\n\nmetric = evaluate.load('seqeval')\n\ndef compute_fbeta(precision, recall, beta=5):\n    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n\n# better compute metrics with f beta and filtering for non O labels\ndef compute_metrics(eval_preds):\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n\n    # Convert logits to label names\n    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [id2label[p] for p, l in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    # Remove 'O' labels from evaluation\n    non_o_true_labels = [[label for label in doc if label != 'O'] for doc in true_labels]\n    non_o_true_predictions = [[pred for pred, true in zip(doc, true_labels[i]) if true != 'O']\n                              for i, doc in enumerate(true_predictions)]\n    \n    # Calculate the required metrics\n    precision = precision_score(non_o_true_labels, non_o_true_predictions)\n    recall = recall_score(non_o_true_labels, non_o_true_predictions)\n    f1 = f1_score(non_o_true_labels, non_o_true_predictions)\n    fbeta = compute_fbeta(precision, recall, beta=5)\n    accuracy = accuracy_score(non_o_true_labels, non_o_true_predictions)\n    \n    # Calculate the required metrics\n    return {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"fbeta\": fbeta,\n        \"accuracy\": accuracy\n    }\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['validation'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:18:05.961479Z","iopub.execute_input":"2024-04-26T18:18:05.962418Z","iopub.status.idle":"2024-04-26T18:18:06.455290Z","shell.execute_reply.started":"2024-04-26T18:18:05.962386Z","shell.execute_reply":"2024-04-26T18:18:06.453656Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Start training\ntrainer.train()\n# api key: 1e55439721e9c3f55077d1b7f5205ccf93924ca6","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:18:10.356962Z","iopub.execute_input":"2024-04-26T18:18:10.357650Z","iopub.status.idle":"2024-04-26T18:26:39.638085Z","shell.execute_reply.started":"2024-04-26T18:18:10.357614Z","shell.execute_reply":"2024-04-26T18:26:39.637160Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1023/1023 08:28, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Fbeta</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.001190</td>\n      <td>0.855556</td>\n      <td>0.729858</td>\n      <td>0.787724</td>\n      <td>0.734005</td>\n      <td>0.823684</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.001100</td>\n      <td>0.000900</td>\n      <td>0.912281</td>\n      <td>0.739336</td>\n      <td>0.816754</td>\n      <td>0.744767</td>\n      <td>0.810526</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000400</td>\n      <td>0.000911</td>\n      <td>0.911765</td>\n      <td>0.734597</td>\n      <td>0.813648</td>\n      <td>0.740129</td>\n      <td>0.813158</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1023, training_loss=0.0007578530171312545, metrics={'train_runtime': 508.6144, 'train_samples_per_second': 32.117, 'train_steps_per_second': 2.011, 'total_flos': 2134642871116800.0, 'train_loss': 0.0007578530171312545, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:26:39.639800Z","iopub.execute_input":"2024-04-26T18:26:39.640082Z","iopub.status.idle":"2024-04-26T18:26:54.556137Z","shell.execute_reply.started":"2024-04-26T18:26:39.640057Z","shell.execute_reply":"2024-04-26T18:26:54.555121Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [86/86 00:13]\n    </div>\n    "},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.0009110897080972791,\n 'eval_precision': 0.9117647058823529,\n 'eval_recall': 0.7345971563981043,\n 'eval_f1': 0.8136482939632546,\n 'eval_fbeta': 0.7401285583103765,\n 'eval_accuracy': 0.8131578947368421,\n 'eval_runtime': 14.9012,\n 'eval_samples_per_second': 91.402,\n 'eval_steps_per_second': 5.771,\n 'epoch': 3.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pipeline","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nnlp = pipeline('ner', model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:49:48.191988Z","iopub.execute_input":"2024-04-26T18:49:48.192751Z","iopub.status.idle":"2024-04-26T18:49:48.402068Z","shell.execute_reply.started":"2024-04-26T18:49:48.192712Z","shell.execute_reply":"2024-04-26T18:49:48.400842Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Analysis","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\ndef error_analysis(predictions, true_labels):\n    errors = Counter()\n    for pred, true in zip(predictions, true_labels):\n        for p, t in zip(pred, true):\n            if p != t:\n                errors[(t, p)] += 1\n    return errors\n\npredictions, true_labels = get_predictions(tokenized_dataset['validation'])\nerror_counts = error_analysis(predictions, true_labels)\n\n# Display the most common errors\nprint(\"Most common misclassifications:\")\nfor (true, pred), count in error_counts.most_common(10):\n    print(f\"True: {true}, Predicted: {pred}, Count: {count}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:53:57.383650Z","iopub.execute_input":"2024-04-26T18:53:57.384008Z","iopub.status.idle":"2024-04-26T18:55:29.275930Z","shell.execute_reply.started":"2024-04-26T18:53:57.383983Z","shell.execute_reply":"2024-04-26T18:55:29.273939Z"},"trusted":true},"execution_count":21,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m                 errors[(t, p)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m errors\n\u001b[0;32m---> 11\u001b[0m predictions, true_labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m error_counts \u001b[38;5;241m=\u001b[39m error_analysis(predictions, true_labels)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Display the most common errors\u001b[39;00m\n","Cell \u001b[0;32mIn[20], line 7\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset)):\n\u001b[1;32m      6\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(dataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     preds \u001b[38;5;241m=\u001b[39m [output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n\u001b[1;32m      9\u001b[0m     labels \u001b[38;5;241m=\u001b[39m [id2label[label] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m dataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:248\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[1;32m    246\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offset_mapping\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1234\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1149\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1148\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1149\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1150\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:285\u001b[0m, in \u001b[0;36mTokenClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    283\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     logits \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: logits,\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecial_tokens_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: special_tokens_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[1;32m    295\u001b[0m }\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:1225\u001b[0m, in \u001b[0;36mDistilBertForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1225\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1235\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1237\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:814\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(input_shape, device\u001b[38;5;241m=\u001b[39mdevice)  \u001b[38;5;66;03m# (bs, seq_length)\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:575\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    567\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    568\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    569\u001b[0m         hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    572\u001b[0m         output_attentions,\n\u001b[1;32m    573\u001b[0m     )\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 575\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:501\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 501\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    510\u001b[0m     sa_output, sa_weights \u001b[38;5;241m=\u001b[39m sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py:237\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    232\u001b[0m mask \u001b[38;5;241m=\u001b[39m (mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mview(mask_reshp)\u001b[38;5;241m.\u001b[39mexpand_as(scores)  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    233\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmasked_fill(\n\u001b[1;32m    234\u001b[0m     mask, torch\u001b[38;5;241m.\u001b[39mtensor(torch\u001b[38;5;241m.\u001b[39mfinfo(scores\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin)\n\u001b[1;32m    235\u001b[0m )  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    238\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(weights)  \u001b[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1856\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1854\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1856\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"# CRF Model","metadata":{}},{"cell_type":"code","source":"!pip install sklearn_crfsuite","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:12:54.543310Z","iopub.execute_input":"2024-04-26T00:12:54.543956Z","iopub.status.idle":"2024-04-26T00:15:24.277191Z","shell.execute_reply.started":"2024-04-26T00:12:54.543922Z","shell.execute_reply":"2024-04-26T00:15:24.276050Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ca33f0671f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sklearn-crfsuite/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ca33f067400>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sklearn-crfsuite/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ca33f0676a0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sklearn-crfsuite/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ca33f067850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sklearn-crfsuite/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ca33f067a00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sklearn-crfsuite/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement sklearn_crfsuite (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for sklearn_crfsuite\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load data\nwith open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\", \"r\") as file:\n    data = json.load(file)\n\n# Data extraction: Keeping tokens and labels grouped by documents\ndocuments = [{'tokens': entry['tokens'], 'labels': entry['labels']} for entry in data]\n\n# Split data into training and validation sets\ntrain_docs, val_docs = train_test_split(documents, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:38:31.295337Z","iopub.execute_input":"2024-04-25T21:38:31.295688Z","iopub.status.idle":"2024-04-25T21:38:35.963135Z","shell.execute_reply.started":"2024-04-25T21:38:31.295659Z","shell.execute_reply":"2024-04-25T21:38:35.962326Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"code","source":"def token_features(token, index, tokens):\n    \"\"\" Generate features for a single token \"\"\"\n    token_lower = token.lower()\n    features = {\n        'bias': 1.0,\n        'token': token,\n        'token.lower()': token_lower,\n        'is_first': index == 0,\n        'is_last': index == len(tokens) - 1,\n        'is_capitalized': token[0].upper() == token[0],\n        'is_all_caps': token.upper() == token,\n        'is_all_lower': token.lower() == token,\n        'prefix-1': token[0],\n        'prefix-2': token[:2] if len(token) > 1 else token[0],\n        'suffix-1': token[-1],\n        'suffix-2': token[-2:] if len(token) > 1 else token[-1],\n        'has_hyphen': '-' in token,\n        'is_numeric': token.isdigit(),\n    }\n    if index > 0:\n        token1 = tokens[index - 1]\n        features.update({\n            '-1:token': token1,\n            '-1:token.lower()': token1.lower(),\n        })\n    if index < len(tokens) - 1:\n        token1 = tokens[index + 1]\n        features.update({\n            '+1:token': token1,\n            '+1:token.lower()': token1.lower(),\n        })\n    return features","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:38:48.177614Z","iopub.execute_input":"2024-04-25T21:38:48.178103Z","iopub.status.idle":"2024-04-25T21:38:48.188421Z","shell.execute_reply.started":"2024-04-25T21:38:48.178072Z","shell.execute_reply":"2024-04-25T21:38:48.187400Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Helper function to generate features for CRF\ndef extract_features(doc):\n    return [token_features(token, i, doc['tokens']) for i, token in enumerate(doc['tokens'])]\n\ndef extract_labels(doc):\n    return doc['labels']\n\nX_train = [extract_features(doc) for doc in train_docs]\ny_train = [extract_labels(doc) for doc in train_docs]\nX_val = [extract_features(doc) for doc in val_docs]\ny_val = [extract_labels(doc) for doc in val_docs]","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:38:51.865660Z","iopub.execute_input":"2024-04-25T21:38:51.866452Z","iopub.status.idle":"2024-04-25T21:39:13.117107Z","shell.execute_reply.started":"2024-04-25T21:38:51.866415Z","shell.execute_reply":"2024-04-25T21:39:13.116330Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"!pip install sklearn_crfsuite","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:39:21.902620Z","iopub.execute_input":"2024-04-25T21:39:21.902968Z","iopub.status.idle":"2024-04-25T21:39:36.028931Z","shell.execute_reply.started":"2024-04-25T21:39:21.902939Z","shell.execute_reply":"2024-04-25T21:39:36.027752Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting sklearn_crfsuite\n  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\nCollecting python-crfsuite>=0.8.3 (from sklearn_crfsuite)\n  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from sklearn_crfsuite) (1.16.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from sklearn_crfsuite) (0.9.0)\nRequirement already satisfied: tqdm>=2.0 in /opt/conda/lib/python3.10/site-packages (from sklearn_crfsuite) (4.66.1)\nDownloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\nDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: python-crfsuite, sklearn_crfsuite\nSuccessfully installed python-crfsuite-0.9.10 sklearn_crfsuite-0.3.6\n","output_type":"stream"}]},{"cell_type":"code","source":"import sklearn_crfsuite\nfrom sklearn_crfsuite import CRF\n    \nclass SafeCRF(CRF):\n    def __repr__(self):\n        return \"SafeCRF()\"\n\n# Initialize and train the CRF with the SafeCRF class\ncrf = SafeCRF(\n    algorithm='lbfgs',\n    c1=0.1,\n    c2=0.1,\n    max_iterations=100,\n    all_possible_transitions=True\n)\n\ntry:\n    crf.fit(X_train, y_train)\nexcept AttributeError:\n    pass","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:39:47.044544Z","iopub.execute_input":"2024-04-25T21:39:47.045210Z","iopub.status.idle":"2024-04-25T21:56:39.996647Z","shell.execute_reply.started":"2024-04-25T21:39:47.045168Z","shell.execute_reply":"2024-04-25T21:56:39.995566Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn_crfsuite import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn_crfsuite.utils import flatten\n\n# Predict on validation data\ny_pred = crf.predict(X_val)\n\n# Determine which labels correspond to non-'O' categories\nlabels = list(crf.classes_)\nnon_o_labels = [label for label in labels if label != 'O']\n\ny_pred_flat = flatten(y_pred)\ny_val_flat = flatten(y_val)\n\n# Since we're working with CRFsuite, ensure labels are handled correctly\nlabel_ids = [labels.index(label) for label in non_o_labels]\n\n# Print classification report excluding 'O' label\nprint(classification_report(\n    y_val_flat, \n    y_pred_flat, \n    labels=non_o_labels,  # Ensure only non-'O' labels are considered\n    target_names=non_o_labels  # This will provide label names in the output\n))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:02:52.438854Z","iopub.execute_input":"2024-04-25T22:02:52.439705Z","iopub.status.idle":"2024-04-25T22:03:31.158825Z","shell.execute_reply.started":"2024-04-25T22:02:52.439672Z","shell.execute_reply":"2024-04-25T22:03:31.157826Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"                  precision    recall  f1-score   support\n\n  B-NAME_STUDENT       0.88      0.65      0.75       263\n  I-NAME_STUDENT       0.91      0.69      0.79       244\n        B-ID_NUM       0.75      0.30      0.43        10\n  B-URL_PERSONAL       0.76      0.68      0.72        28\n         B-EMAIL       1.00      0.67      0.80         3\n      B-USERNAME       0.00      0.00      0.00         0\n     B-PHONE_NUM       0.00      0.00      0.00         2\n     I-PHONE_NUM       0.00      0.00      0.00         3\n  I-URL_PERSONAL       0.00      0.00      0.00         0\n        I-ID_NUM       0.00      0.00      0.00         0\nB-STREET_ADDRESS       0.00      0.00      0.00         1\nI-STREET_ADDRESS       0.00      0.00      0.00        10\n\n       micro avg       0.89      0.64      0.75       564\n       macro avg       0.36      0.25      0.29       564\n    weighted avg       0.86      0.64      0.74       564\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate F1 score for non-'O' labels\nf1_score_non_o = metrics.flat_f1_score(y_val, y_pred, average='weighted', labels=non_o_labels)\nprint(f\"F1 Score for Non-'O' labels: {f1_score_non_o}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:03:44.180535Z","iopub.execute_input":"2024-04-25T22:03:44.181216Z","iopub.status.idle":"2024-04-25T22:03:51.717530Z","shell.execute_reply.started":"2024-04-25T22:03:44.181184Z","shell.execute_reply":"2024-04-25T22:03:51.716570Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"F1 Score for Non-'O' labels: 0.7352858761368722\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Precision and Recall for Non-'O' labels\nprecision = metrics.flat_precision_score(y_val, y_pred, average='weighted', labels=non_o_labels)\nrecall = metrics.flat_recall_score(y_val, y_pred, average='weighted', labels=non_o_labels)\n\nprint(f\"Precision for Non-'O' labels: {precision}\")\nprint(f\"Recall for Non-'O' labels: {recall}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:03:51.719441Z","iopub.execute_input":"2024-04-25T22:03:51.720059Z","iopub.status.idle":"2024-04-25T22:04:06.848459Z","shell.execute_reply.started":"2024-04-25T22:03:51.720023Z","shell.execute_reply":"2024-04-25T22:04:06.847551Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Precision for Non-'O' labels: 0.8602719466780521\nRecall for Non-'O' labels: 0.6436170212765957\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import fbeta_score\n\n# Calculate F beta score with beta=5 (recall weighted more heavily than precision)\nf_beta = fbeta_score(y_val_flat, y_pred_flat, labels=non_o_labels, beta=5, average='weighted')\n\nprint(f\"F-beta score with beta=5 for non-'O' labels: {f_beta}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:04:06.849994Z","iopub.execute_input":"2024-04-25T22:04:06.850297Z","iopub.status.idle":"2024-04-25T22:04:14.319009Z","shell.execute_reply.started":"2024-04-25T22:04:06.850269Z","shell.execute_reply":"2024-04-25T22:04:14.318110Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"F-beta score with beta=5 for non-'O' labels: 0.6498175749001541\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Error Analysis and Explainability","metadata":{}},{"cell_type":"code","source":"from sklearn_crfsuite.utils import flatten\n\n# Flatten the predictions and true labels\ny_pred_flat = flatten(y_pred)\ny_val_flat = flatten(y_val)\n\n# Extracting tokens for the validation set\n# Assuming val_docs is a list of dictionaries with 'tokens' and 'labels' keys\ntokens_val = [doc['tokens'] for doc in val_docs]  # List of lists of tokens for validation docs\nlabels_val = [doc['labels'] for doc in val_docs]  # List of lists of labels for validation docs\n\n# Now flatten these for direct comparisons in mismatches (if needed)\ntokens_val_flat = flatten(tokens_val)\nlabels_val_flat = flatten(labels_val)\n\n\n# Find indices where predictions and true values differ\nmismatches = [i for i, (y_pred, y_true) in enumerate(zip(y_pred_flat, y_val_flat)) if y_pred != y_true]\n\n# Print some mismatches for review\nprint(\"Showing some mismatches:\")\nfor i in mismatches[:20]:  # Show first 10 mismatches\n    print(f\"Token: '{tokens_val_flat[i]}', Predicted: '{y_pred_flat[i]}', True: '{y_val_flat[i]}'\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:27:00.450511Z","iopub.execute_input":"2024-04-25T22:27:00.451340Z","iopub.status.idle":"2024-04-25T22:27:00.689801Z","shell.execute_reply.started":"2024-04-25T22:27:00.451305Z","shell.execute_reply":"2024-04-25T22:27:00.688757Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"5884\nShowing some mismatches:\nToken: 'Tony', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Flores', Predicted: 'O', True: 'I-NAME_STUDENT'\nToken: 'Tony', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Flores', Predicted: 'O', True: 'I-NAME_STUDENT'\nToken: 'Hussain', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Mohammed', Predicted: 'B-NAME_STUDENT', True: 'I-NAME_STUDENT'\nToken: 'Hussain', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Mohammed', Predicted: 'B-NAME_STUDENT', True: 'I-NAME_STUDENT'\nToken: 'Nweze', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Stanley', Predicted: 'O', True: 'I-NAME_STUDENT'\nToken: 'Sjoerd', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Van', Predicted: 'O', True: 'I-NAME_STUDENT'\nToken: 'Der', Predicted: 'O', True: 'I-NAME_STUDENT'\nToken: 'Wal', Predicted: 'O', True: 'I-NAME_STUDENT'\nToken: 'Sergio', Predicted: 'B-NAME_STUDENT', True: 'O'\nToken: 'Cruz', Predicted: 'I-NAME_STUDENT', True: 'O'\nToken: 'Easyblood', Predicted: 'B-NAME_STUDENT', True: 'O'\nToken: 'System', Predicted: 'I-NAME_STUDENT', True: 'O'\nToken: 'Marwa', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Maria', Predicted: 'B-NAME_STUDENT', True: 'O'\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn_crfsuite.utils import flatten\n\n# Function to explain predictions for a specific document\ndef explain_prediction(index):\n    doc = X_val[index]\n    true_labels = y_val[index]\n    pred_labels = y_pred[index]\n\n    print(\"Token\\tTrue\\tPred\\tFeatures\")\n    for token, true_label, pred_label in zip(doc, true_labels, pred_labels):\n        # Only display explanations for errors or upon specific conditions\n        if true_label != pred_label:\n            features = [f\"{k}={v}\" for k, v in token.items()]\n            print(f\"{token['token']}\\t{true_label}\\t{pred_label}\\t{' '.join(features)}\\n\")\n\n# Example usage: Explain predictions for the first document where an error occurs\nfor i in range(len(X_val)):\n    if any(true_label != pred_label for true_label, pred_label in zip(y_val[i], y_pred[i])):\n        print(f\"Errors in document {i}:\")\n        explain_prediction(i)\n        break # remove to explain more than 1 document","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:38:33.885788Z","iopub.execute_input":"2024-04-25T22:38:33.886516Z","iopub.status.idle":"2024-04-25T22:38:33.895419Z","shell.execute_reply.started":"2024-04-25T22:38:33.886483Z","shell.execute_reply":"2024-04-25T22:38:33.894428Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Errors in document 8:\nToken\tTrue\tPred\tFeatures\nTony\tB-NAME_STUDENT\tO\tbias=1.0 token=Tony token.lower()=tony is_first=True is_last=False is_capitalized=True is_all_caps=False is_all_lower=False prefix-1=T prefix-2=To suffix-1=y suffix-2=ny has_hyphen=False is_numeric=False +1:token=Flores +1:token.lower()=flores\n\nFlores\tI-NAME_STUDENT\tO\tbias=1.0 token=Flores token.lower()=flores is_first=False is_last=False is_capitalized=True is_all_caps=False is_all_lower=False prefix-1=F prefix-2=Fl suffix-1=s suffix-2=es has_hyphen=False is_numeric=False -1:token=Tony -1:token.lower()=tony +1:token=| +1:token.lower()=|\n\nTony\tB-NAME_STUDENT\tO\tbias=1.0 token=Tony token.lower()=tony is_first=False is_last=False is_capitalized=True is_all_caps=False is_all_lower=False prefix-1=T prefix-2=To suffix-1=y suffix-2=ny has_hyphen=False is_numeric=False -1:token=\n\n -1:token.lower()=\n\n +1:token=Flores +1:token.lower()=flores\n\nFlores\tI-NAME_STUDENT\tO\tbias=1.0 token=Flores token.lower()=flores is_first=False is_last=False is_capitalized=True is_all_caps=False is_all_lower=False prefix-1=F prefix-2=Fl suffix-1=s suffix-2=es has_hyphen=False is_numeric=False -1:token=Tony -1:token.lower()=tony +1:token=| +1:token.lower()=|\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Submission on test data","metadata":{}},{"cell_type":"code","source":"# Load data\nwith open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\", \"r\") as file:\n    test_data = json.load(file)\n\n# Data extraction: Keeping tokens and labels grouped by documents\ntest_documents = [{'tokens': entry['tokens'], 'document': entry['document']} for entry in test_data]","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:54:48.273463Z","iopub.execute_input":"2024-04-25T22:54:48.274265Z","iopub.status.idle":"2024-04-25T22:54:48.282505Z","shell.execute_reply.started":"2024-04-25T22:54:48.274230Z","shell.execute_reply":"2024-04-25T22:54:48.281464Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def extract_features_for_test(doc):\n    return [token_features(token, i, doc['tokens']) for i, token in enumerate(doc['tokens'])]\n\nX_test = [extract_features_for_test(doc) for doc in test_documents]","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:54:50.971662Z","iopub.execute_input":"2024-04-25T22:54:50.972032Z","iopub.status.idle":"2024-04-25T22:54:51.013954Z","shell.execute_reply.started":"2024-04-25T22:54:50.972001Z","shell.execute_reply":"2024-04-25T22:54:51.013073Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# Assuming 'crf' is your trained CRF model\ny_pred_test = [crf.predict_single(xseq) for xseq in X_test]","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:54:59.126871Z","iopub.execute_input":"2024-04-25T22:54:59.127518Z","iopub.status.idle":"2024-04-25T22:54:59.297953Z","shell.execute_reply.started":"2024-04-25T22:54:59.127476Z","shell.execute_reply":"2024-04-25T22:54:59.296988Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"import csv\n\n# Create submission.csv file\nwith open('/kaggle/working/submission.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['row_id', 'document', 'token', 'label'])\n    \n    row_id = 0\n    for doc_idx, (doc, pred_labels) in enumerate(zip(test_documents, y_pred_test)):\n        document_id = doc['document']\n        for token_idx, label in enumerate(pred_labels):\n            if label != 'O':  # We include only PII labels\n                writer.writerow([row_id, document_id, token_idx, label])\n                row_id += 1","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:59:25.264238Z","iopub.execute_input":"2024-04-25T22:59:25.264913Z","iopub.status.idle":"2024-04-25T22:59:25.272869Z","shell.execute_reply.started":"2024-04-25T22:59:25.264880Z","shell.execute_reply":"2024-04-25T22:59:25.271842Z"},"trusted":true},"execution_count":73,"outputs":[]}]}