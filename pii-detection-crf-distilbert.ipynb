{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7659420,"sourceType":"datasetVersion","datasetId":4459964}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CS 475/675 Machine Learning: Project\n## Goals:\n### 4.1 Must accomplish\n- Implement a robust data preprocessing pipeline to handle tokenization, feature extraction, and label encoding.\n- Develop and train a machine learning model capable of accurately detecting PII types in student essays, achieving a competitive score on the evaluation metric.\n- Generate predictions for the test set essays and submit them in the required format for evaluation.\n\n### 4.2 Expect to accomplish\n- Fine-tune the model architecture and hyperparameters to optimize performance on the provided training data.\n- Conduct error analysis and model interpretation to identify common misclassifications and areas for improvement.\n- Investigate the use of external datasets or pre-trained language models to enhance the model’s generalization capabilities.\n\n### 4.3 Would like to accomplish\n- Implement ensemble learning techniques, such as model averaging or stacking, to combine multiple base models and further boost detection accuracy and robustness.\n- Investigate methods for handling imbalance class distributions, particularly for rare PII types.\n- Develop visualization tools and techniques to facilitate the interpretation of model predictions.\n","metadata":{}},{"cell_type":"markdown","source":"# PreTrained DistilBERT Model","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers datasets accelerate","metadata":{"execution":{"iopub.status.busy":"2024-04-30T02:43:23.085875Z","iopub.execute_input":"2024-04-30T02:43:23.086780Z","iopub.status.idle":"2024-04-30T02:44:05.048694Z","shell.execute_reply.started":"2024-04-30T02:43:23.086742Z","shell.execute_reply":"2024-04-30T02:44:05.047685Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.2)\nCollecting transformers\n  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Downloading datasets-2.19.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\nCollecting accelerate\n  Downloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.21.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nCollecting tokenizers<0.20,>=0.19 (from transformers)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nCollecting pyarrow>=12.0.0 (from datasets)\n  Downloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nCollecting pyarrow-hotfix (from datasets)\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-2.19.0-py3-none-any.whl (542 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, pyarrow, tokenizers, accelerate, transformers, datasets\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 11.0.0\n    Uninstalling pyarrow-11.0.0:\n      Successfully uninstalled pyarrow-11.0.0\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.28.0\n    Uninstalling accelerate-0.28.0:\n      Successfully uninstalled accelerate-0.28.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.2\n    Uninstalling transformers-4.38.2:\n      Successfully uninstalled transformers-4.38.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.5 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 16.0.0 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.29.3 datasets-2.19.0 pyarrow-16.0.0 pyarrow-hotfix-0.6 tokenizers-0.19.1 transformers-4.40.1\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data loading","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict, load_metric\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification","metadata":{"execution":{"iopub.status.busy":"2024-04-30T02:45:59.478636Z","iopub.execute_input":"2024-04-30T02:45:59.479410Z","iopub.status.idle":"2024-04-30T02:46:18.197241Z","shell.execute_reply.started":"2024-04-30T02:45:59.479373Z","shell.execute_reply":"2024-04-30T02:46:18.196404Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-30 02:46:10.235767: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 02:46:10.235870: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 02:46:10.365902: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport torch\nfrom transformers import DistilBertTokenizerFast\nfrom sklearn.model_selection import train_test_split\n\n# Load your dataset\nwith open('/kaggle/input/pii-detection-removal-from-educational-data/train.json', 'r') as file:\n    data = json.load(file)\n    \n# Extract tokens and labels from your data\ntokens = [entry['tokens'] for entry in data]\nlabels = [entry['labels'] for entry in data]\n\n# Create a dictionary for the Dataset\ndata_dict = {\n    'tokens': tokens,\n    'labels': labels\n}","metadata":{"execution":{"iopub.status.busy":"2024-04-30T02:46:18.198907Z","iopub.execute_input":"2024-04-30T02:46:18.199612Z","iopub.status.idle":"2024-04-30T02:46:20.840372Z","shell.execute_reply.started":"2024-04-30T02:46:18.199584Z","shell.execute_reply":"2024-04-30T02:46:20.839549Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Convert to Hugging Face dataset format\ndataset = Dataset.from_dict(data_dict)\n\n# Split the dataset into training and validation sets\ntrain_test_split = dataset.train_test_split(test_size=0.2)\ndataset = DatasetDict(train=train_test_split['train'], validation=train_test_split['test'])\n\n# Define labels and map them\nlabel_list = ['B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', \n               'B-PHONE_NUM', 'B-STREET_ADDRESS', 'B-URL_PERSONAL', \n               'B-USERNAME', 'I-ID_NUM', 'I-NAME_STUDENT', \n               'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL', 'O']\nlabel2id = {label: i for i, label in enumerate(label_list)}\nid2label = {i: label for label, i in label2id.items()}","metadata":{"execution":{"iopub.status.busy":"2024-04-30T02:46:20.841520Z","iopub.execute_input":"2024-04-30T02:46:20.841872Z","iopub.status.idle":"2024-04-30T02:46:22.666582Z","shell.execute_reply.started":"2024-04-30T02:46:20.841839Z","shell.execute_reply":"2024-04-30T02:46:22.665774Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(id2label)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T02:46:22.668542Z","iopub.execute_input":"2024-04-30T02:46:22.668847Z","iopub.status.idle":"2024-04-30T02:46:22.673639Z","shell.execute_reply.started":"2024-04-30T02:46:22.668817Z","shell.execute_reply":"2024-04-30T02:46:22.672546Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_checkpoint = \"distilbert-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(\n        examples['tokens'], \n        max_length=512,  # Set maximum sequence length (DistilBERT's typical max)\n        truncation=True,  # Ensure truncation is applied\n        padding='max_length',  # Apply padding\n        is_split_into_words=True\n    )\n    \n    labels = []\n    for i, doc_labels in enumerate(examples['labels']):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_id = None\n        label_ids = []\n        for word_id in word_ids:\n            # Only add labels for non-special tokens and avoid duplicated labels\n            if word_id is None:\n                label_ids.append(-100)  # For special tokens like [CLS], [SEP], [PAD]\n            elif word_id != previous_word_id:\n                label_ids.append(label2id.get(doc_labels[word_id], -100))\n            else:\n                # Use -100 for repeated subword tokens to ignore during loss calculation\n                label_ids.append(-100) \n            previous_word_id = word_id\n        labels.append(label_ids)\n        \n    tokenized_inputs['labels'] = labels\n    return tokenized_inputs\n\n\n# Apply tokenization and label alignment\ntokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True, \n                                remove_columns=dataset['train'].column_names)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T02:46:22.674804Z","iopub.execute_input":"2024-04-30T02:46:22.675069Z","iopub.status.idle":"2024-04-30T02:46:53.923337Z","shell.execute_reply.started":"2024-04-30T02:46:22.675046Z","shell.execute_reply":"2024-04-30T02:46:53.922363Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c47d7295aff42c888112786493bbefc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf02eb4c38e5418e8199a984a46c1fdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"331962d7ff094417960a3dad7e083377"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be21ade3f73141ecb6cce854823977f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5445 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ae757df6e0342fc82e8e02740cf0aa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1362 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad6eee08bb884e7ea79db40d005c7e17"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Collation and Model Building","metadata":{}},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, \n                                                        num_labels=len(label_list), \n                                                        id2label=id2label, label2id=label2id)\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    evaluation_strategy='epoch',\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=10,\n    weight_decay=0.01\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:00:47.717590Z","iopub.execute_input":"2024-04-30T03:00:47.718698Z","iopub.status.idle":"2024-04-30T03:00:48.030182Z","shell.execute_reply.started":"2024-04-30T03:00:47.718656Z","shell.execute_reply":"2024-04-30T03:00:48.029132Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training Metrics","metadata":{}},{"cell_type":"code","source":"!pip install evaluate\n!pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:00:50.971123Z","iopub.execute_input":"2024-04-30T03:00:50.971749Z","iopub.status.idle":"2024-04-30T03:01:17.414922Z","shell.execute_reply.started":"2024-04-30T03:00:50.971717Z","shell.execute_reply":"2024-04-30T03:01:17.413745Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.1)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.19.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.21.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.0.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: seqeval in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Metrics for evaluation\nimport evaluate\nfrom seqeval.metrics import precision_score, recall_score, f1_score\nfrom seqeval.metrics import classification_report, accuracy_score\n\nmetric = evaluate.load('seqeval')\n\ndef compute_fbeta(precision, recall, beta=5):\n    return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n\n# better compute metrics with f beta and filtering for non O labels\ndef compute_metrics(eval_preds):\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n\n    # Convert logits to label names\n    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n    true_predictions = [\n        [id2label[p] for p, l in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    # Remove 'O' labels from evaluation\n    non_o_true_labels = [[label for label in doc if label != 'O'] for doc in true_labels]\n    non_o_true_predictions = [[pred for pred, true in zip(doc, true_labels[i]) if true != 'O']\n                              for i, doc in enumerate(true_predictions)]\n    \n    # Calculate the required metrics\n    precision = precision_score(non_o_true_labels, non_o_true_predictions)\n    recall = recall_score(non_o_true_labels, non_o_true_predictions)\n    f1 = f1_score(non_o_true_labels, non_o_true_predictions)\n    fbeta = compute_fbeta(precision, recall, beta=5)\n    accuracy = accuracy_score(non_o_true_labels, non_o_true_predictions)\n    \n    # Calculate the required metrics\n    return {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"fbeta\": fbeta,\n        \"accuracy\": accuracy\n    }\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['validation'],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:01:17.417344Z","iopub.execute_input":"2024-04-30T03:01:17.417744Z","iopub.status.idle":"2024-04-30T03:01:17.808669Z","shell.execute_reply.started":"2024-04-30T03:01:17.417716Z","shell.execute_reply":"2024-04-30T03:01:17.807721Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Start training\ntrainer.train()\n# api key: 1e55439721e9c3f55077d1b7f5205ccf93924ca6","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:01:17.810040Z","iopub.execute_input":"2024-04-30T03:01:17.810414Z","iopub.status.idle":"2024-04-30T03:29:34.421437Z","shell.execute_reply.started":"2024-04-30T03:01:17.810378Z","shell.execute_reply":"2024-04-30T03:29:34.420470Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3410' max='3410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3410/3410 28:15, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Fbeta</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.001598</td>\n      <td>0.836364</td>\n      <td>0.565574</td>\n      <td>0.674817</td>\n      <td>0.572706</td>\n      <td>0.626126</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.029400</td>\n      <td>0.000983</td>\n      <td>0.934066</td>\n      <td>0.696721</td>\n      <td>0.798122</td>\n      <td>0.703598</td>\n      <td>0.770270</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000900</td>\n      <td>0.000888</td>\n      <td>0.919048</td>\n      <td>0.790984</td>\n      <td>0.850220</td>\n      <td>0.795246</td>\n      <td>0.876126</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000900</td>\n      <td>0.000839</td>\n      <td>0.891089</td>\n      <td>0.737705</td>\n      <td>0.807175</td>\n      <td>0.742621</td>\n      <td>0.826577</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000400</td>\n      <td>0.000874</td>\n      <td>0.906863</td>\n      <td>0.758197</td>\n      <td>0.825893</td>\n      <td>0.763008</td>\n      <td>0.840090</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.000200</td>\n      <td>0.000861</td>\n      <td>0.933649</td>\n      <td>0.807377</td>\n      <td>0.865934</td>\n      <td>0.811599</td>\n      <td>0.869369</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.000200</td>\n      <td>0.000979</td>\n      <td>0.909548</td>\n      <td>0.741803</td>\n      <td>0.817156</td>\n      <td>0.747103</td>\n      <td>0.806306</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.000200</td>\n      <td>0.000987</td>\n      <td>0.936275</td>\n      <td>0.782787</td>\n      <td>0.852679</td>\n      <td>0.787754</td>\n      <td>0.835586</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.000100</td>\n      <td>0.001037</td>\n      <td>0.925000</td>\n      <td>0.758197</td>\n      <td>0.833333</td>\n      <td>0.763492</td>\n      <td>0.817568</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.000100</td>\n      <td>0.001032</td>\n      <td>0.909091</td>\n      <td>0.778689</td>\n      <td>0.838852</td>\n      <td>0.783008</td>\n      <td>0.844595</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3410, training_loss=0.004589451325079563, metrics={'train_runtime': 1695.9104, 'train_samples_per_second': 32.107, 'train_steps_per_second': 2.011, 'total_flos': 7115476237056000.0, 'train_loss': 0.004589451325079563, 'epoch': 10.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2024-04-30T03:29:34.423847Z","iopub.execute_input":"2024-04-30T03:29:34.424247Z","iopub.status.idle":"2024-04-30T03:29:49.401441Z","shell.execute_reply.started":"2024-04-30T03:29:34.424208Z","shell.execute_reply":"2024-04-30T03:29:49.400360Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [86/86 00:13]\n    </div>\n    "},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.0010316807311028242,\n 'eval_precision': 0.9090909090909091,\n 'eval_recall': 0.7786885245901639,\n 'eval_f1': 0.8388520971302428,\n 'eval_fbeta': 0.7830084006974164,\n 'eval_accuracy': 0.8445945945945946,\n 'eval_runtime': 14.9603,\n 'eval_samples_per_second': 91.041,\n 'eval_steps_per_second': 5.749,\n 'epoch': 10.0}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pipeline","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nnlp = pipeline('ner', model=model, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T18:49:48.191988Z","iopub.execute_input":"2024-04-26T18:49:48.192751Z","iopub.status.idle":"2024-04-26T18:49:48.402068Z","shell.execute_reply.started":"2024-04-26T18:49:48.192712Z","shell.execute_reply":"2024-04-26T18:49:48.400842Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# CRF Model","metadata":{}},{"cell_type":"code","source":"!pip install sklearn_crfsuite","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:12:54.543310Z","iopub.execute_input":"2024-04-26T00:12:54.543956Z","iopub.status.idle":"2024-04-26T00:15:24.277191Z","shell.execute_reply.started":"2024-04-26T00:12:54.543922Z","shell.execute_reply":"2024-04-26T00:15:24.276050Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ca33f0671f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sklearn-crfsuite/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ca33f067400>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sklearn-crfsuite/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ca33f0676a0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sklearn-crfsuite/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ca33f067850>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sklearn-crfsuite/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7ca33f067a00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/sklearn-crfsuite/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement sklearn_crfsuite (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for sklearn_crfsuite\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load data\nwith open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\", \"r\") as file:\n    data = json.load(file)\n\n# Data extraction: Keeping tokens and labels grouped by documents\ndocuments = [{'tokens': entry['tokens'], 'labels': entry['labels']} for entry in data]\n\n# Split data into training and validation sets\ntrain_docs, val_docs = train_test_split(documents, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:38:31.295337Z","iopub.execute_input":"2024-04-25T21:38:31.295688Z","iopub.status.idle":"2024-04-25T21:38:35.963135Z","shell.execute_reply.started":"2024-04-25T21:38:31.295659Z","shell.execute_reply":"2024-04-25T21:38:35.962326Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"cell_type":"code","source":"def token_features(token, index, tokens):\n    \"\"\" Generate features for a single token \"\"\"\n    token_lower = token.lower()\n    features = {\n        'bias': 1.0,\n        'token': token,\n        'token.lower()': token_lower,\n        'is_first': index == 0,\n        'is_last': index == len(tokens) - 1,\n        'is_capitalized': token[0].upper() == token[0],\n        'is_all_caps': token.upper() == token,\n        'is_all_lower': token.lower() == token,\n        'prefix-1': token[0],\n        'prefix-2': token[:2] if len(token) > 1 else token[0],\n        'suffix-1': token[-1],\n        'suffix-2': token[-2:] if len(token) > 1 else token[-1],\n        'has_hyphen': '-' in token,\n        'is_numeric': token.isdigit(),\n    }\n    if index > 0:\n        token1 = tokens[index - 1]\n        features.update({\n            '-1:token': token1,\n            '-1:token.lower()': token1.lower(),\n        })\n    if index < len(tokens) - 1:\n        token1 = tokens[index + 1]\n        features.update({\n            '+1:token': token1,\n            '+1:token.lower()': token1.lower(),\n        })\n    return features","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:38:48.177614Z","iopub.execute_input":"2024-04-25T21:38:48.178103Z","iopub.status.idle":"2024-04-25T21:38:48.188421Z","shell.execute_reply.started":"2024-04-25T21:38:48.178072Z","shell.execute_reply":"2024-04-25T21:38:48.187400Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Helper function to generate features for CRF\ndef extract_features(doc):\n    return [token_features(token, i, doc['tokens']) for i, token in enumerate(doc['tokens'])]\n\ndef extract_labels(doc):\n    return doc['labels']\n\nX_train = [extract_features(doc) for doc in train_docs]\ny_train = [extract_labels(doc) for doc in train_docs]\nX_val = [extract_features(doc) for doc in val_docs]\ny_val = [extract_labels(doc) for doc in val_docs]","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:38:51.865660Z","iopub.execute_input":"2024-04-25T21:38:51.866452Z","iopub.status.idle":"2024-04-25T21:39:13.117107Z","shell.execute_reply.started":"2024-04-25T21:38:51.866415Z","shell.execute_reply":"2024-04-25T21:39:13.116330Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"!pip install sklearn_crfsuite","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:39:21.902620Z","iopub.execute_input":"2024-04-25T21:39:21.902968Z","iopub.status.idle":"2024-04-25T21:39:36.028931Z","shell.execute_reply.started":"2024-04-25T21:39:21.902939Z","shell.execute_reply":"2024-04-25T21:39:36.027752Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting sklearn_crfsuite\n  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\nCollecting python-crfsuite>=0.8.3 (from sklearn_crfsuite)\n  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from sklearn_crfsuite) (1.16.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from sklearn_crfsuite) (0.9.0)\nRequirement already satisfied: tqdm>=2.0 in /opt/conda/lib/python3.10/site-packages (from sklearn_crfsuite) (4.66.1)\nDownloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\nDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: python-crfsuite, sklearn_crfsuite\nSuccessfully installed python-crfsuite-0.9.10 sklearn_crfsuite-0.3.6\n","output_type":"stream"}]},{"cell_type":"code","source":"import sklearn_crfsuite\nfrom sklearn_crfsuite import CRF\n    \nclass SafeCRF(CRF):\n    def __repr__(self):\n        return \"SafeCRF()\"\n\n# Initialize and train the CRF with the SafeCRF class\ncrf = SafeCRF(\n    algorithm='lbfgs',\n    c1=0.1,\n    c2=0.1,\n    max_iterations=100,\n    all_possible_transitions=True\n)\n\ntry:\n    crf.fit(X_train, y_train)\nexcept AttributeError:\n    pass","metadata":{"execution":{"iopub.status.busy":"2024-04-25T21:39:47.044544Z","iopub.execute_input":"2024-04-25T21:39:47.045210Z","iopub.status.idle":"2024-04-25T21:56:39.996647Z","shell.execute_reply.started":"2024-04-25T21:39:47.045168Z","shell.execute_reply":"2024-04-25T21:56:39.995566Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn_crfsuite import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn_crfsuite.utils import flatten\n\n# Predict on validation data\ny_pred = crf.predict(X_val)\n\n# Determine which labels correspond to non-'O' categories\nlabels = list(crf.classes_)\nnon_o_labels = [label for label in labels if label != 'O']\n\ny_pred_flat = flatten(y_pred)\ny_val_flat = flatten(y_val)\n\n# Since we're working with CRFsuite, ensure labels are handled correctly\nlabel_ids = [labels.index(label) for label in non_o_labels]\n\n# Print classification report excluding 'O' label\nprint(classification_report(\n    y_val_flat, \n    y_pred_flat, \n    labels=non_o_labels,  # Ensure only non-'O' labels are considered\n    target_names=non_o_labels  # This will provide label names in the output\n))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:02:52.438854Z","iopub.execute_input":"2024-04-25T22:02:52.439705Z","iopub.status.idle":"2024-04-25T22:03:31.158825Z","shell.execute_reply.started":"2024-04-25T22:02:52.439672Z","shell.execute_reply":"2024-04-25T22:03:31.157826Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"                  precision    recall  f1-score   support\n\n  B-NAME_STUDENT       0.88      0.65      0.75       263\n  I-NAME_STUDENT       0.91      0.69      0.79       244\n        B-ID_NUM       0.75      0.30      0.43        10\n  B-URL_PERSONAL       0.76      0.68      0.72        28\n         B-EMAIL       1.00      0.67      0.80         3\n      B-USERNAME       0.00      0.00      0.00         0\n     B-PHONE_NUM       0.00      0.00      0.00         2\n     I-PHONE_NUM       0.00      0.00      0.00         3\n  I-URL_PERSONAL       0.00      0.00      0.00         0\n        I-ID_NUM       0.00      0.00      0.00         0\nB-STREET_ADDRESS       0.00      0.00      0.00         1\nI-STREET_ADDRESS       0.00      0.00      0.00        10\n\n       micro avg       0.89      0.64      0.75       564\n       macro avg       0.36      0.25      0.29       564\n    weighted avg       0.86      0.64      0.74       564\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate F1 score for non-'O' labels\nf1_score_non_o = metrics.flat_f1_score(y_val, y_pred, average='weighted', labels=non_o_labels)\nprint(f\"F1 Score for Non-'O' labels: {f1_score_non_o}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:03:44.180535Z","iopub.execute_input":"2024-04-25T22:03:44.181216Z","iopub.status.idle":"2024-04-25T22:03:51.717530Z","shell.execute_reply.started":"2024-04-25T22:03:44.181184Z","shell.execute_reply":"2024-04-25T22:03:51.716570Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"F1 Score for Non-'O' labels: 0.7352858761368722\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n","output_type":"stream"}]},{"cell_type":"code","source":"# Precision and Recall for Non-'O' labels\nprecision = metrics.flat_precision_score(y_val, y_pred, average='weighted', labels=non_o_labels)\nrecall = metrics.flat_recall_score(y_val, y_pred, average='weighted', labels=non_o_labels)\n\nprint(f\"Precision for Non-'O' labels: {precision}\")\nprint(f\"Recall for Non-'O' labels: {recall}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:03:51.719441Z","iopub.execute_input":"2024-04-25T22:03:51.720059Z","iopub.status.idle":"2024-04-25T22:04:06.848459Z","shell.execute_reply.started":"2024-04-25T22:03:51.720023Z","shell.execute_reply":"2024-04-25T22:04:06.847551Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Precision for Non-'O' labels: 0.8602719466780521\nRecall for Non-'O' labels: 0.6436170212765957\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import fbeta_score\n\n# Calculate F beta score with beta=5 (recall weighted more heavily than precision)\nf_beta = fbeta_score(y_val_flat, y_pred_flat, labels=non_o_labels, beta=5, average='weighted')\n\nprint(f\"F-beta score with beta=5 for non-'O' labels: {f_beta}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:04:06.849994Z","iopub.execute_input":"2024-04-25T22:04:06.850297Z","iopub.status.idle":"2024-04-25T22:04:14.319009Z","shell.execute_reply.started":"2024-04-25T22:04:06.850269Z","shell.execute_reply":"2024-04-25T22:04:14.318110Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"F-beta score with beta=5 for non-'O' labels: 0.6498175749001541\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Error Analysis and Explainability","metadata":{}},{"cell_type":"code","source":"from sklearn_crfsuite.utils import flatten\n\n# Flatten the predictions and true labels\ny_pred_flat = flatten(y_pred)\ny_val_flat = flatten(y_val)\n\n# Extracting tokens for the validation set\n# Assuming val_docs is a list of dictionaries with 'tokens' and 'labels' keys\ntokens_val = [doc['tokens'] for doc in val_docs]  # List of lists of tokens for validation docs\nlabels_val = [doc['labels'] for doc in val_docs]  # List of lists of labels for validation docs\n\n# Now flatten these for direct comparisons in mismatches (if needed)\ntokens_val_flat = flatten(tokens_val)\nlabels_val_flat = flatten(labels_val)\n\n\n# Find indices where predictions and true values differ\nmismatches = [i for i, (y_pred, y_true) in enumerate(zip(y_pred_flat, y_val_flat)) if y_pred != y_true]\n\n# Print some mismatches for review\nprint(\"Showing some mismatches:\")\nfor i in mismatches[:20]:  # Show first 10 mismatches\n    print(f\"Token: '{tokens_val_flat[i]}', Predicted: '{y_pred_flat[i]}', True: '{y_val_flat[i]}'\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:27:00.450511Z","iopub.execute_input":"2024-04-25T22:27:00.451340Z","iopub.status.idle":"2024-04-25T22:27:00.689801Z","shell.execute_reply.started":"2024-04-25T22:27:00.451305Z","shell.execute_reply":"2024-04-25T22:27:00.688757Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"5884\nShowing some mismatches:\nToken: 'Tony', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Flores', Predicted: 'O', True: 'I-NAME_STUDENT'\nToken: 'Tony', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Flores', Predicted: 'O', True: 'I-NAME_STUDENT'\nToken: 'Hussain', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Mohammed', Predicted: 'B-NAME_STUDENT', True: 'I-NAME_STUDENT'\nToken: 'Hussain', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Mohammed', Predicted: 'B-NAME_STUDENT', True: 'I-NAME_STUDENT'\nToken: 'Nweze', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Stanley', Predicted: 'O', True: 'I-NAME_STUDENT'\nToken: 'Sjoerd', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Van', Predicted: 'O', True: 'I-NAME_STUDENT'\nToken: 'Der', Predicted: 'O', True: 'I-NAME_STUDENT'\nToken: 'Wal', Predicted: 'O', True: 'I-NAME_STUDENT'\nToken: 'Sergio', Predicted: 'B-NAME_STUDENT', True: 'O'\nToken: 'Cruz', Predicted: 'I-NAME_STUDENT', True: 'O'\nToken: 'Easyblood', Predicted: 'B-NAME_STUDENT', True: 'O'\nToken: 'System', Predicted: 'I-NAME_STUDENT', True: 'O'\nToken: 'Marwa', Predicted: 'O', True: 'B-NAME_STUDENT'\nToken: 'Maria', Predicted: 'B-NAME_STUDENT', True: 'O'\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn_crfsuite.utils import flatten\n\n# Function to explain predictions for a specific document\ndef explain_prediction(index):\n    doc = X_val[index]\n    true_labels = y_val[index]\n    pred_labels = y_pred[index]\n\n    print(\"Token\\tTrue\\tPred\\tFeatures\")\n    for token, true_label, pred_label in zip(doc, true_labels, pred_labels):\n        # Only display explanations for errors or upon specific conditions\n        if true_label != pred_label:\n            features = [f\"{k}={v}\" for k, v in token.items()]\n            print(f\"{token['token']}\\t{true_label}\\t{pred_label}\\t{' '.join(features)}\\n\")\n\n# Example usage: Explain predictions for the first document where an error occurs\nfor i in range(len(X_val)):\n    if any(true_label != pred_label for true_label, pred_label in zip(y_val[i], y_pred[i])):\n        print(f\"Errors in document {i}:\")\n        explain_prediction(i)\n        break # remove to explain more than 1 document","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:38:33.885788Z","iopub.execute_input":"2024-04-25T22:38:33.886516Z","iopub.status.idle":"2024-04-25T22:38:33.895419Z","shell.execute_reply.started":"2024-04-25T22:38:33.886483Z","shell.execute_reply":"2024-04-25T22:38:33.894428Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Errors in document 8:\nToken\tTrue\tPred\tFeatures\nTony\tB-NAME_STUDENT\tO\tbias=1.0 token=Tony token.lower()=tony is_first=True is_last=False is_capitalized=True is_all_caps=False is_all_lower=False prefix-1=T prefix-2=To suffix-1=y suffix-2=ny has_hyphen=False is_numeric=False +1:token=Flores +1:token.lower()=flores\n\nFlores\tI-NAME_STUDENT\tO\tbias=1.0 token=Flores token.lower()=flores is_first=False is_last=False is_capitalized=True is_all_caps=False is_all_lower=False prefix-1=F prefix-2=Fl suffix-1=s suffix-2=es has_hyphen=False is_numeric=False -1:token=Tony -1:token.lower()=tony +1:token=| +1:token.lower()=|\n\nTony\tB-NAME_STUDENT\tO\tbias=1.0 token=Tony token.lower()=tony is_first=False is_last=False is_capitalized=True is_all_caps=False is_all_lower=False prefix-1=T prefix-2=To suffix-1=y suffix-2=ny has_hyphen=False is_numeric=False -1:token=\n\n -1:token.lower()=\n\n +1:token=Flores +1:token.lower()=flores\n\nFlores\tI-NAME_STUDENT\tO\tbias=1.0 token=Flores token.lower()=flores is_first=False is_last=False is_capitalized=True is_all_caps=False is_all_lower=False prefix-1=F prefix-2=Fl suffix-1=s suffix-2=es has_hyphen=False is_numeric=False -1:token=Tony -1:token.lower()=tony +1:token=| +1:token.lower()=|\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Submission on test data","metadata":{}},{"cell_type":"code","source":"# Load data\nwith open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\", \"r\") as file:\n    test_data = json.load(file)\n\n# Data extraction: Keeping tokens and labels grouped by documents\ntest_documents = [{'tokens': entry['tokens'], 'document': entry['document']} for entry in test_data]","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:54:48.273463Z","iopub.execute_input":"2024-04-25T22:54:48.274265Z","iopub.status.idle":"2024-04-25T22:54:48.282505Z","shell.execute_reply.started":"2024-04-25T22:54:48.274230Z","shell.execute_reply":"2024-04-25T22:54:48.281464Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def extract_features_for_test(doc):\n    return [token_features(token, i, doc['tokens']) for i, token in enumerate(doc['tokens'])]\n\nX_test = [extract_features_for_test(doc) for doc in test_documents]","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:54:50.971662Z","iopub.execute_input":"2024-04-25T22:54:50.972032Z","iopub.status.idle":"2024-04-25T22:54:51.013954Z","shell.execute_reply.started":"2024-04-25T22:54:50.972001Z","shell.execute_reply":"2024-04-25T22:54:51.013073Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# Assuming 'crf' is your trained CRF model\ny_pred_test = [crf.predict_single(xseq) for xseq in X_test]","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:54:59.126871Z","iopub.execute_input":"2024-04-25T22:54:59.127518Z","iopub.status.idle":"2024-04-25T22:54:59.297953Z","shell.execute_reply.started":"2024-04-25T22:54:59.127476Z","shell.execute_reply":"2024-04-25T22:54:59.296988Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"import csv\n\n# Create submission.csv file\nwith open('/kaggle/working/submission.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow(['row_id', 'document', 'token', 'label'])\n    \n    row_id = 0\n    for doc_idx, (doc, pred_labels) in enumerate(zip(test_documents, y_pred_test)):\n        document_id = doc['document']\n        for token_idx, label in enumerate(pred_labels):\n            if label != 'O':  # We include only PII labels\n                writer.writerow([row_id, document_id, token_idx, label])\n                row_id += 1","metadata":{"execution":{"iopub.status.busy":"2024-04-25T22:59:25.264238Z","iopub.execute_input":"2024-04-25T22:59:25.264913Z","iopub.status.idle":"2024-04-25T22:59:25.272869Z","shell.execute_reply.started":"2024-04-25T22:59:25.264880Z","shell.execute_reply":"2024-04-25T22:59:25.271842Z"},"trusted":true},"execution_count":73,"outputs":[]}]}